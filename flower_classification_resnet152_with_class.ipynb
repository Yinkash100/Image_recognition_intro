{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flower_classification_resnet152_with_class.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "l3vQiaNnCjcK",
        "colab_type": "code",
        "outputId": "2e71138b-b360-4b60-b42f-4d638f5386cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "!pip install PIL\n",
        "!pip install image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (5.4.1)\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (5.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OAJEs22nZfMp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#helps Colab install torch\n",
        "\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45OOaaCpaIzk",
        "colab_type": "code",
        "outputId": "7daf78c9-d8e8-4417-fe5a-2d9a4b8e3f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#import the required packages\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iuEXn0FqvMMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!wget \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/September/5baa60a0_flower-photos/flower-photos.zip\" -P \"drive/My Drive/Colab_Notebooks/pytorch_challenge\"\n",
        "#!unzip \"drive/My Drive/Colab_Notebooks/pytorch_challenge/flower-photos.zip\" -d \"drive/My Drive/Colab_Notebooks/pytorch_challenge/flower\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Udj631wUqKCq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "2qKbzyQHK8d6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls \"drive/My Drive/Colab_Notebooks/pytorch_challenge\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "smkDiUQGcHtQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "data_dir = 'drive/My Drive/Colab_Notebooks/pytorch_challenge/new_flower_classification_model/photos/flower_data'\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 25\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.25\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(45),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                             [0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "                                       \n",
        "test_transforms = transforms.Compose([\n",
        "        transforms.Resize(224 + 32),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                             [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/valid', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "# choose the training and test datasets\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)\n",
        "\n",
        "# specify the image classes\n",
        "classes = [n for n in range(1,103)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wINb-7kzVCCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls '/content/drive/My Drive/Colab_Notebooks'\n",
        "\n",
        "#cwd = os.getcwd()\n",
        "#print(cwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkE0BaTIxDlR",
        "colab_type": "code",
        "outputId": "7533b7e3-0864-41d0-faeb-8e1b443d91eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(50, 8))\n",
        "# display 20 images\n",
        "for idx in np.arange(10):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# helper function to un-normalize and display an image\\ndef imshow(img):\\n    img = img / 2 + 0.5  # unnormalize\\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\\n    \\n# obtain one batch of training images\\ndataiter = iter(train_loader)\\nimages, labels = dataiter.next()\\nimages = images.numpy() # convert images to numpy for display\\n\\n# plot the images in the batch, along with the corresponding labels\\nfig = plt.figure(figsize=(50, 8))\\n# display 20 images\\nfor idx in np.arange(10):\\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\\n    imshow(images[idx])\\n    ax.set_title(classes[labels[idx]])\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0omWd4DhjSh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "I1dZLK_x_0Ch",
        "colab_type": "code",
        "outputId": "6c0b7691-c9ef-4f0d-9bd6-86dd0b88a527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet152(pretrained=True)\n",
        "input_features = model.fc.in_features\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "for i , param in enumerate(model.parameters()):\n",
        "    if(i>300):\n",
        "      param.requires_grad=True\n",
        "\n",
        "print(input_features)\n",
        "#creating our own classifier\n",
        "classifier = nn.Sequential(nn.Linear(input_features, 512),\n",
        "                            nn.ReLU(),\n",
        "                            #nn.Dropout(0.3),\n",
        "                            nn.Linear(512, 102),\n",
        "                            nn.LogSoftmax(dim=1))\n",
        "     \n",
        "model.fc = classifier\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "epoch = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.torch/models/resnet152-b121ed2d.pth\n",
            "100%|██████████| 241530880/241530880 [00:13<00:00, 18574049.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bTn7r-H8tb2v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#To Continue Training or To use the saved model (resnet_keep_training)\n",
        "#model = resnet152(*args, **kwargs)\n",
        "#optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load('drive/My Drive/Colab_Notebooks/pytorch_challenge/flower_classification_models/resnet152_keep_training.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "#model.eval()\n",
        "# - or -\n",
        "model.train()\n",
        "model = model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztn0y_MJ_2eE",
        "colab_type": "code",
        "outputId": "085b838d-c677-449d-d847-ab40ea86df53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 100\n",
        "#checkpoint_path = \"flower_classification_models/resnet_18.pt\"\n",
        "\n",
        "validation_loss = []\n",
        "training_loss = []\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(epoch, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    exp_lr_scheduler.step()\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "\n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
        "    validation_loss.append(valid_loss)\n",
        "    training_loss.append(train_loss)\n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('lr = {:.6f} Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(optimizer.param_groups[0]['lr'], valid_loss_min,valid_loss))\n",
        "        torch.save(model.state_dict(), 'drive/My Drive/Colab_Notebooks/pytorch_challenge/flower_classification_models/resnet152.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'drive/My Drive/Colab_Notebooks/pytorch_challenge/flower_classification_models/resnet152_keep_training.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4836b27f9e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TzD41c6kExiv",
        "colab_type": "code",
        "outputId": "9a9f9424-8739-4305-9f74-604336493e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1819
        }
      },
      "cell_type": "code",
      "source": [
        "#load the last saved model\n",
        "#model.load_state_dict(torch.load('drive/My Drive/Colab_Notebooks/pytorch_challenge/flower_classification_models/resnet152.pt'))\n",
        "checkpoint = torch.load('drive/My Drive/Colab_Notebooks/pytorch_challenge/flower_classification_models/resnet152_keep_training.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#'drive/My Drive/Colab_Notebooks/pytorch_challenge/flower_classification_models/resnet152_keep_training.pt'\n",
        "\n",
        "\n",
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(102))\n",
        "class_total = list(0. for i in range(102))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(18):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(102):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.075710\n",
            "\n",
            "Test Accuracy of     1: 100% ( 8/ 8)\n",
            "Test Accuracy of     2: 100% ( 4/ 4)\n",
            "Test Accuracy of     3: 100% ( 6/ 6)\n",
            "Test Accuracy of     4: N/A (no training examples)\n",
            "Test Accuracy of     5: 100% ( 4/ 4)\n",
            "Test Accuracy of     6: 100% (10/10)\n",
            "Test Accuracy of     7: 100% ( 4/ 4)\n",
            "Test Accuracy of     8: N/A (no training examples)\n",
            "Test Accuracy of     9: N/A (no training examples)\n",
            "Test Accuracy of    10: 100% ( 7/ 7)\n",
            "Test Accuracy of    11: 100% ( 2/ 2)\n",
            "Test Accuracy of    12: 100% ( 9/ 9)\n",
            "Test Accuracy of    13: 100% (11/11)\n",
            "Test Accuracy of    14: 100% ( 4/ 4)\n",
            "Test Accuracy of    15: 100% ( 3/ 3)\n",
            "Test Accuracy of    16: 100% ( 3/ 3)\n",
            "Test Accuracy of    17: 100% ( 4/ 4)\n",
            "Test Accuracy of    18: 100% ( 8/ 8)\n",
            "Test Accuracy of    19: 100% ( 5/ 5)\n",
            "Test Accuracy of    20: 100% ( 5/ 5)\n",
            "Test Accuracy of    21: 100% ( 2/ 2)\n",
            "Test Accuracy of    22: 100% ( 3/ 3)\n",
            "Test Accuracy of    23: 100% ( 1/ 1)\n",
            "Test Accuracy of    24: 100% ( 5/ 5)\n",
            "Test Accuracy of    25: N/A (no training examples)\n",
            "Test Accuracy of    26: 100% ( 2/ 2)\n",
            "Test Accuracy of    27: 100% (10/10)\n",
            "Test Accuracy of    28: 100% ( 2/ 2)\n",
            "Test Accuracy of    29: 100% ( 3/ 3)\n",
            "Test Accuracy of    30: 100% ( 1/ 1)\n",
            "Test Accuracy of    31: 100% ( 6/ 6)\n",
            "Test Accuracy of    32: 100% ( 4/ 4)\n",
            "Test Accuracy of    33: 66% ( 4/ 6)\n",
            "Test Accuracy of    34: 100% ( 2/ 2)\n",
            "Test Accuracy of    35: 100% ( 3/ 3)\n",
            "Test Accuracy of    36: 100% ( 3/ 3)\n",
            "Test Accuracy of    37: 66% ( 4/ 6)\n",
            "Test Accuracy of    38: 100% ( 5/ 5)\n",
            "Test Accuracy of    39: 100% ( 9/ 9)\n",
            "Test Accuracy of    40: 100% ( 6/ 6)\n",
            "Test Accuracy of    41: 85% ( 6/ 7)\n",
            "Test Accuracy of    42: 100% ( 9/ 9)\n",
            "Test Accuracy of    43: 100% ( 4/ 4)\n",
            "Test Accuracy of    44: 100% (11/11)\n",
            "Test Accuracy of    45: 100% ( 3/ 3)\n",
            "Test Accuracy of    46: 100% ( 6/ 6)\n",
            "Test Accuracy of    47: 100% ( 4/ 4)\n",
            "Test Accuracy of    48: 100% ( 7/ 7)\n",
            "Test Accuracy of    49: 100% ( 7/ 7)\n",
            "Test Accuracy of    50: 100% (18/18)\n",
            "Test Accuracy of    51: 100% (10/10)\n",
            "Test Accuracy of    52: 87% ( 7/ 8)\n",
            "Test Accuracy of    53: 100% ( 4/ 4)\n",
            "Test Accuracy of    54: 100% ( 8/ 8)\n",
            "Test Accuracy of    55: 100% ( 6/ 6)\n",
            "Test Accuracy of    56: 100% ( 2/ 2)\n",
            "Test Accuracy of    57: 100% (14/14)\n",
            "Test Accuracy of    58: 100% ( 2/ 2)\n",
            "Test Accuracy of    59: N/A (no training examples)\n",
            "Test Accuracy of    60: 100% (10/10)\n",
            "Test Accuracy of    61: 100% ( 6/ 6)\n",
            "Test Accuracy of    62: 50% ( 1/ 2)\n",
            "Test Accuracy of    63: 100% ( 2/ 2)\n",
            "Test Accuracy of    64: 100% ( 5/ 5)\n",
            "Test Accuracy of    65: 100% ( 7/ 7)\n",
            "Test Accuracy of    66: 100% ( 4/ 4)\n",
            "Test Accuracy of    67: N/A (no training examples)\n",
            "Test Accuracy of    68: 100% ( 5/ 5)\n",
            "Test Accuracy of    69: 100% ( 5/ 5)\n",
            "Test Accuracy of    70: 100% ( 1/ 1)\n",
            "Test Accuracy of    71: 100% ( 7/ 7)\n",
            "Test Accuracy of    72: N/A (no training examples)\n",
            "Test Accuracy of    73: 100% ( 6/ 6)\n",
            "Test Accuracy of    74: 100% (12/12)\n",
            "Test Accuracy of    75: 100% (15/15)\n",
            "Test Accuracy of    76: 100% ( 5/ 5)\n",
            "Test Accuracy of    77: 93% (15/16)\n",
            "Test Accuracy of    78: 100% (18/18)\n",
            "Test Accuracy of    79: 100% ( 4/ 4)\n",
            "Test Accuracy of    80: 100% ( 4/ 4)\n",
            "Test Accuracy of    81: 100% ( 5/ 5)\n",
            "Test Accuracy of    82: 100% ( 5/ 5)\n",
            "Test Accuracy of    83: 100% (18/18)\n",
            "Test Accuracy of    84: 83% ( 5/ 6)\n",
            "Test Accuracy of    85: 100% (12/12)\n",
            "Test Accuracy of    86: 100% ( 4/ 4)\n",
            "Test Accuracy of    87: 100% ( 5/ 5)\n",
            "Test Accuracy of    88: 100% ( 5/ 5)\n",
            "Test Accuracy of    89: 100% ( 4/ 4)\n",
            "Test Accuracy of    90: 94% (17/18)\n",
            "Test Accuracy of    91: 100% (11/11)\n",
            "Test Accuracy of    92: 100% ( 3/ 3)\n",
            "Test Accuracy of    93: 100% ( 2/ 2)\n",
            "Test Accuracy of    94: 100% ( 2/ 2)\n",
            "Test Accuracy of    95: 100% ( 2/ 2)\n",
            "Test Accuracy of    96: 83% ( 5/ 6)\n",
            "Test Accuracy of    97: 100% (10/10)\n",
            "Test Accuracy of    98: 90% ( 9/10)\n",
            "Test Accuracy of    99: 100% ( 8/ 8)\n",
            "Test Accuracy of   100: 100% ( 2/ 2)\n",
            "Test Accuracy of   101: 100% (10/10)\n",
            "Test Accuracy of   102: 100% ( 6/ 6)\n",
            "\n",
            "Test Accuracy (Overall): 97% (582/594)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6C_8M5SBFC1O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(training_loss, label='Training loss')\n",
        "plt.plot(validation_loss, label='validation_loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}